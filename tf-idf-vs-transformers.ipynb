{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e662c18",
   "metadata": {},
   "source": [
    "### Apply Chollet criteria\n",
    "\n",
    "When you're working on a task where data is Text **one question** that could arise is:\n",
    "\n",
    "***Is it worth using a model based on Tranformers or is it possible to use a simpler model, for example based on TF-IDF?***\n",
    "\n",
    "Well, it depends on the data you have for your task. In general, **Deep Learning** enables to get an higher accuracy if you have **many samples**, and more is better.\n",
    "\n",
    "In **Deep Learning in Python Book v2** a simple **criteria** is suggested: you need to consider\n",
    "* the number of samples you have\n",
    "* the average length (in words) of samples\n",
    "\n",
    "Then, you can compute the Ratio\n",
    "\n",
    "Ratio = N_SAMPLES/AVG_LENGTH\n",
    "\n",
    "If this Ratio is < 1500, it is suggested to go with a simpler model\n",
    "\n",
    "But thar criteria was established in 2017 and in Deep Learning field things are changing very rapidly. \n",
    "\n",
    "So I decided to test the criteria, using **IMdb dataset** for a **Sentiment Analysis** task. As you can see from this NB, in the case of IMdb dataset\n",
    "\n",
    "**Ratio = 173**\n",
    "\n",
    "In addition to the book: **Deep Learning in Python v2**, see also (section 2.5):\n",
    "\n",
    "https://developers.google.com/machine-learning/guides/text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a0fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1bde5",
   "metadata": {},
   "source": [
    "### loads the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ad9ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [01:34<00:00, 530.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# reads all the files to build the entire dataset as DataFrame\n",
    "# code inspired by: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "# it takes some time... good to have a progress bar\n",
    "# added using tqdm\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "NUM_FILES = 50000\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=NUM_FILES) as pbar:\n",
    "    for s in ('test', 'train'):\n",
    "        for l in ('pos', 'neg'):\n",
    "            path = os.path.join(basepath, s, l)\n",
    "            for file in sorted(os.listdir(path)):\n",
    "                with open(os.path.join(path, file), \n",
    "                          'r', encoding='utf-8') as infile:\n",
    "                    txt = infile.read()\n",
    "                df = df.append([[txt, labels[l]]], \n",
    "                               ignore_index=True)\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "# rename columns as expected by transformers\n",
    "df.columns = ['text', 'target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86b15a",
   "metadata": {},
   "source": [
    "### compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c81c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 40000.0\n",
      "Avg number of words per sentence: 231.2\n",
      "\n",
      "Computed ratio is: 173.0\n"
     ]
    }
   ],
   "source": [
    "# we assume here that only 80% of the dataset will be used for training, the rest for validation\n",
    "VALID_FRAC = 0.2\n",
    "\n",
    "NUM_TRAIN_SAMPLES = df.shape[0] * (1. - VALID_FRAC)\n",
    "\n",
    "# compute avg # of words per sentence\n",
    "df[\"wps\"] = df[\"text\"].str.split().apply(len)\n",
    "avg_wps = round(df.describe()['wps']['mean'], 1)\n",
    "\n",
    "ratio = round(NUM_TRAIN_SAMPLES/avg_wps, 1)\n",
    "\n",
    "print(f'Number of training samples: {NUM_TRAIN_SAMPLES}')\n",
    "print(f'Avg number of words per sentence: {avg_wps}')\n",
    "print()\n",
    "print(f'Computed ratio is: {ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6423aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio is < 1500, so based on the expressed criteria we should go with a TF-IDF model.\n",
    "# But, in reality, today with resources available to everyone, using a Trasnformer a better result in terms of accuracy can be obtained.\n",
    "# Have a look at the other Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed48a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_p37_gpu_v2]",
   "language": "python",
   "name": "conda-env-nlp_p37_gpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
